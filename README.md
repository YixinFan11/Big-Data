# Big-Data
### Introduction
<details>
  <summary>Definition</summary>
  
1. The Volume(data at rest): In data mining, datasets have seen an enormous increase in the number of examples and features, and unfortunately it’s not like you could currently have a 1 petabyte drive on your laptop/desktop computer to process it.
2. The Velocity(data in motion): the speed at which the data comes to us, and at which we have to process the data if we want to take advantage of it!(real-time decision)
3. The Variety(data in many forms): Many kinds of data with different formats and structure: 
     - traditional structured data(such as tables and relational databases) for which we know the schema
     - semi-structure data such as XML files for which we still can infer the schema
     - completely unstructured data such as text, images, audio, video for which we don’t know the schema and might want to impose some structure prior to doing any analyses.
4. The Veracity(data in doubt): Problem of trusting your data. Uncertainty about the quality of the data due to various data sources. Data that may be missing, be ambiguous, or simply put, be wrong.
5. The Value(the central V, data in use): Bigger datasets enable people to find better patterns and better insights about the problem. However, More data doesn’t necessarily imply better results.
</details>

<details>
  <summary>Problems and Solutions</summary>
  
- Scalability to big data sets
  - Divide-and-Conquer
  - Scale-up(Vertical sclability): Add more memory, processors
  - Scale-out(Horizontal sclability): Add more (cheaper) nodes
Big data technologies are based on scale-out
</details>

### Section I: MapReduce

























